{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re  \n",
    "import nltk\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config = dict(\n",
    "    search_tweets_api = dict(\n",
    "        account_type = 'premium',\n",
    "        endpoint = 'https://api.twitter.com/1.1/tweets/search/30day/KitchenAid.json',\n",
    "        consumer_key = 'foyCkjGvs1JSBVW3iyoffhITm',\n",
    "        consumer_secret = 'aeDuwdyFt86cFfN8sia8tHkiIExSsQjwvusdetk5AguUojnG2z'\n",
    "    )\n",
    ")\n",
    "\n",
    "with open('twitter_keys_30days.yaml', 'w') as config_file:\n",
    "    yaml.dump(config, config_file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\searchtweets\\credentials.py:34: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  search_creds = yaml.load(f)[yaml_key]\n",
      "Grabbing bearer token from OAUTH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bearer_token': 'AAAAAAAAAAAAAAAAAAAAAEamAwEAAAAAq91t%2BQg1%2BH5aPOp254JFGXeyRas%3Do2eu2ThehePIfHSE5ZUI3hExxJdqFsG9VtltO4XJ3m9On7dqGf', 'endpoint': 'https://api.twitter.com/1.1/tweets/search/30day/KitchenAid.json', 'extra_headers_dict': None}\n"
     ]
    }
   ],
   "source": [
    "from searchtweets import load_credentials\n",
    "\n",
    "premium_search_args = load_credentials(\"twitter_keys_30days.yaml\",\n",
    "                                       yaml_key=\"search_tweets_api\",\n",
    "                                       env_overwrite=False)\n",
    "print(premium_search_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import gen_rule_payload\n",
    "\n",
    "rule = gen_rule_payload(\"kitchenaid\", \n",
    "                        results_per_call=100#,\n",
    "                        #from_date=\"2019-11-01 07:15\",\n",
    "                        #to_date=\"2019-11-31 23:11\"\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResultStream: \n",
      "\t{\n",
      "    \"username\": null,\n",
      "    \"endpoint\": \"https://api.twitter.com/1.1/tweets/search/30day/KitchenAid.json\",\n",
      "    \"rule_payload\": {\n",
      "        \"query\": \"kitchenaid\",\n",
      "        \"maxResults\": 100\n",
      "    },\n",
      "    \"tweetify\": true,\n",
      "    \"max_results\": 500\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from searchtweets import ResultStream\n",
    "\n",
    "rs = ResultStream(rule_payload=rule,\n",
    "                  max_results=500,\n",
    "                  **premium_search_args)\n",
    "print(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: Thu Dec 19 19:55:16 +0000 2019\n",
      "20: Thu Dec 19 19:10:14 +0000 2019\n",
      "30: Thu Dec 19 18:17:29 +0000 2019\n",
      "40: Thu Dec 19 17:31:03 +0000 2019\n",
      "50: Thu Dec 19 16:57:02 +0000 2019\n",
      "60: Thu Dec 19 16:34:49 +0000 2019\n",
      "70: Thu Dec 19 15:48:48 +0000 2019\n",
      "80: Thu Dec 19 14:45:36 +0000 2019\n",
      "90: Thu Dec 19 14:14:14 +0000 2019\n",
      "100: Thu Dec 19 13:28:08 +0000 2019\n",
      "110: Thu Dec 19 12:47:52 +0000 2019\n",
      "120: Thu Dec 19 11:38:39 +0000 2019\n",
      "130: Thu Dec 19 10:30:00 +0000 2019\n",
      "140: Thu Dec 19 08:59:11 +0000 2019\n",
      "150: Thu Dec 19 08:11:04 +0000 2019\n",
      "160: Thu Dec 19 06:05:57 +0000 2019\n",
      "170: Thu Dec 19 04:58:37 +0000 2019\n",
      "180: Thu Dec 19 03:51:12 +0000 2019\n",
      "190: Thu Dec 19 03:08:38 +0000 2019\n",
      "200: Thu Dec 19 02:10:47 +0000 2019\n",
      "210: Thu Dec 19 01:28:39 +0000 2019\n",
      "220: Thu Dec 19 00:34:19 +0000 2019\n",
      "230: Wed Dec 18 23:36:33 +0000 2019\n",
      "240: Wed Dec 18 22:32:04 +0000 2019\n",
      "250: Wed Dec 18 21:43:54 +0000 2019\n",
      "260: Wed Dec 18 20:59:24 +0000 2019\n",
      "270: Wed Dec 18 20:27:42 +0000 2019\n",
      "280: Wed Dec 18 20:02:44 +0000 2019\n",
      "290: Wed Dec 18 19:26:54 +0000 2019\n",
      "300: Wed Dec 18 18:44:46 +0000 2019\n",
      "310: Wed Dec 18 18:04:30 +0000 2019\n",
      "320: Wed Dec 18 16:51:28 +0000 2019\n",
      "330: Wed Dec 18 16:09:58 +0000 2019\n",
      "340: Wed Dec 18 15:41:10 +0000 2019\n",
      "350: Wed Dec 18 15:24:52 +0000 2019\n",
      "360: Wed Dec 18 14:35:31 +0000 2019\n",
      "370: Wed Dec 18 13:36:33 +0000 2019\n",
      "380: Wed Dec 18 12:53:52 +0000 2019\n",
      "390: Wed Dec 18 12:30:33 +0000 2019\n",
      "400: Wed Dec 18 12:16:36 +0000 2019\n",
      "410: Wed Dec 18 11:37:10 +0000 2019\n",
      "420: Wed Dec 18 10:50:18 +0000 2019\n",
      "430: Wed Dec 18 09:43:53 +0000 2019\n",
      "440: Wed Dec 18 08:22:49 +0000 2019\n",
      "450: Wed Dec 18 06:22:56 +0000 2019\n",
      "460: Wed Dec 18 04:47:24 +0000 2019\n",
      "470: Wed Dec 18 03:58:24 +0000 2019\n",
      "480: Wed Dec 18 03:00:00 +0000 2019\n",
      "490: Wed Dec 18 01:39:22 +0000 2019\n",
      "500: Wed Dec 18 01:19:24 +0000 2019\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('test1.jsonl', 'a', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for tweet in rs.stream():\n",
    "        n += 1\n",
    "        if n % 10 == 0:\n",
    "            print('{0}: {1}'.format(str(n), tweet['created_at']))\n",
    "        json.dump(tweet, f)\n",
    "        f.write('\\n')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_lines\n",
    "\n",
    "def load_jsonl(file):\n",
    "    tweets = []\n",
    "    with open(file, 'rb') as f:\n",
    "        for tweet in json_lines.reader(f, broken=True):\n",
    "            reduced_tweet = {\n",
    "                'username' : tweet['user']['screen_name'],\n",
    "                'status_text' : tweet['text'],\n",
    "                'hashtags' : tweet['entities']['hashtags'],\n",
    "                \n",
    "                }\n",
    "                \n",
    "            tweets.append(reduced_tweet)\n",
    "    return (tweets)\n",
    "\n",
    "tweets = load_jsonl('test1.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'richardson_ceri',\n",
       " 'status_text': 'RT @WithFranca: ✨INSTAGRAM GIVEAWAY✨ Our Big #Giveaway is now live! “WIN a KitchenAid Artisan Stand Mixer 4.8L bowl in Matte Grey worth a w…',\n",
       " 'hashtags': [{'text': 'Giveaway', 'indices': [45, 54]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML Tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "    \n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test1.jsonl\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(\"test1.jsonl\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        if line.strip(\"\\n\") != \"nickname_to_delete\":\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test1.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = preprocess(tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'richardson_ceri',\n",
       " 'status_text': 'RT @WithFranca: ✨INSTAGRAM GIVEAWAY✨ Our Big #Giveaway is now live! “WIN a KitchenAid Artisan Stand Mixer 4.8L bowl in Matte Grey worth a w…',\n",
       " 'hashtags': [{'text': 'Giveaway', 'indices': [45, 54]}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('KitchenAid', 2047), ('a', 1943), ('…', 1921), ('.', 1900), (':', 1878), ('!', 1813), ('-', 1356), ('RT', 1206), ('the', 1118), ('to', 1092)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "fname = 'test1.jsonl'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('KitchenAid', 2047), ('…', 1921), ('RT', 1206), ('Sue', 1064), ('Mixer', 712), ('✨', 648), ('Artisan', 563), ('Stand', 548), ('Toronto', 532), ('attack', 532)]\n"
     ]
    }
   ],
   "source": [
    "with open('test1.jsonl', 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_stop)\n",
    "# Print the first 5 most frequent words\n",
    "print(count_all.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('KitchenAid', 2047), ('…', 1921), ('RT', 1206), ('Sue', 1064), ('Mixer', 712), ('✨', 648), ('Artisan', 563), ('Stand', 548), ('Toronto', 532), ('attack', 532)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(count_all.most_common(10))\n",
    "fname = 'test1.jsonl'\n",
    "with open(fname, 'r') as f:\n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "       # Count terms only once, equivalent to Document Frequency\n",
    "    terms_single = set(terms_all)\n",
    "    # Count hashtags only\n",
    "    terms_hash = [term for term in preprocess(tweet['text']) \n",
    "                  if term.startswith('#')]\n",
    "    # Update the counter\n",
    "    count_all.update(terms_hash)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sue', 4), ('Toronto', 2), ('attack', 2), ('RT', 1), (\"Let's\", 1), ('Play', 1), ('Innocent', 1), ('Smith', 1), ('amp', 1), ('Wesson', 1)]\n"
     ]
    }
   ],
   "source": [
    "terms_single = set(terms_all)\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs\n",
    "count_all.update(terms_only)\n",
    "print(count_all.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams \n",
    " \n",
    "terms_bigram = bigrams(terms_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
